---
title: "Assignment2"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(
  message    = FALSE,   
  warning    = FALSE,   
  fig.align  = "center",
  fig.width  = 4.5,       
  fig.height = 3,
  dpi        = 120,
  out.width  = "70%",   
  fig.show   = "hold"   
)
```

```{r, include=FALSE}
# Clear plots
if(!is.null(dev.list())) dev.off()

# Clean workspace
rm(list=ls())
```

# 1. Data preparation

The first step is to check the dataset, to understand which variables we can work with, and set their datatypes accordingly.

```{r}
# Loading data
df <- read.csv("aug_train.csv", header=T)
summary(df)

```
We will set the datatypes accordingly.

After checking the dataset, we observe that many variable's observation's are left in blank, we set it to NA because it's missing data.

```{r}
# Converting characters to factor
df[sapply(df, is.character)] <- lapply(df[sapply(df, is.character)], as.factor)

# Converting binary columns (only 0 or 1) to factor
df[sapply(df, function(x) all(na.omit(unique(x)) %in% c(0, 1)))] <- 
  lapply(df[sapply(df, function(x) all(na.omit(unique(x)) %in% c(0, 1)))], as.factor)

# Setting NAs
df[df == ""] <- NA
summary(df)
str(df)
```

After a first inspection, we have decided to remove `enrollee_id`and `city` from our working dataset, given that `enrollee_id` is an unnecessary id for each observation (after checking that there are no duplicates) and that `city` is a factor with a very high number of levels (123), whose information is contained in the `city_development_index` variable as can be seen by the distinct combination of city and citydevelopment index.

### enrollee_id
We confirm that `enrollee_id` does not have duplicates or missing data, as we wanted, because it's the variable that identifies each enrollee. 

But this variable is not necessary for modelling because it doesn't have predictive power, so we remove it.

```{r}
sum(duplicated(df$enrollee_id))
sum(is.na(df$enrollee_id))
```


```{r}

library(dplyr)
nrow(distinct(df, df$city, df$city_development_index))
```


```{r}
df <- df[,-c(1,2)]
str(df)
```



First, we check the missing data and zeros of the variables to see which ones are the most affected.
Then, we also check the count of missing values per individual, to see which observations are not reliable (those with a high % of missing values), to discuss whether to remove them. 

!!!!!!! Duplicates: after removing enrollee_id we found 60 observations that contain the same information. We need to check if they actually are or its just that they share the same exact profile.

```{r}
any(duplicated(df))
sum(duplicated(df))

dup_rows <- df %>% 
  filter(duplicated(.) | duplicated(., fromLast = TRUE))

nrow(dup_rows)


# Variables
check_na <- function(x) {
  na = sum(is.na(x))
  }
results <- lapply(df, check_na)
results

# Individuals
table(rowSums(is.na(df)))


```






-----------------------------------------------------------
After analyzing the individuals, we have observed that some of them have more than 30% missing values. (missing values in more than 3 of the 12 total variables).

We have decided to select that as a threshold, and only keep the observations that don't have too many missing values, therefore removing 878 observations (4,5%).

## OBSERVATION REMOVAL FOR A LOT OF NAs

```{r}
sum(rowSums(is.na(df)) >= 4)
df <- df[rowSums(is.na(df)) < 4, ]
```
For the rest of the missing values, since they don't represent a very big loss of information, we will analyze more in depth in the following sections:
  - Missingness mechanism (MCAR...)
  - Missingness per variable (some have around 30% of missing values)
  

We decided to analyze this by checking the profiling of a new variable containing the number of NAs for each observation, to try to identify patterns.

```{r}

```

  
  
  
## Numerical variables


### city_development_index
The variable `city_development_index` has no NA and no severe outliers, only mild outliers, but we won't remove them.
```{r}
summary(df$city_development_index)
hist(df$city_development_index)

# Outliers
boxplot(df$city_development_index, horizontal = T)
varout <- summary(df$city_development_index)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
umout <- varout[5]+1.5*iqr
lmout <- varout[2]-1.5*iqr

sev_out <- which((df$city_development_index > usout) | (df$city_development_index < lsout))
mild_out <- which((df$city_development_index > umout) | (df$city_development_index < lmout))

boxplot(df$city_development_index, horizontal = T)
abline(v=usout,col='orange', add = T)
abline(v=lsout,col='orange', add = T)
abline(v=umout,col='orange', add = T)
abline(v=lmout,col='orange', add = T)
length(sev_out) #0
length(mild_out) #17

# Distribution
hist(df$city_development_index, freq = F)
m = mean(df$city_development_index)
std = sd(df$city_development_index)
curve(dnorm(x,m,std),add=T, col="red")
```



### training_hours
The variable `training_hours` does not have any NA.

```{r}
summary(df$training_hours)
hist(df$training_hours)

# Outliers
boxplot(df$training_hours, horizontal = T)
varout <- summary(df$training_hours)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
umout <- varout[5]+1.5*iqr
lmout <- varout[2]-1.5*iqr

sev_out <- which((df$training_hours > usout) | (df$training_hours < lsout))
length(sev_out) #275
mild_out <- which((df$training_hours > umout) | (df$training_hours < lmout))
length(mild_out) #984

boxplot(df$training_hours, horizontal = T)
abline(v=usout,col='orange', add = T)
abline(v=lsout,col='orange', add = T)
abline(v=umout,col='orange', add = T)
abline(v=lmout,col='orange', add = T)
sev_out #0
mild_out #0

# Distribution
hist(df$training_hours, freq = F)
m = mean(df$training_hours)
std = sd(df$training_hours)
curve(dnorm(x,m,std),add=T, col="red")
```

We see that the variable is left-skewed, so in order to properly address outlying observations, we will analyze the log-transformation of the variable.

We can see how it does not have any extreme outliers, and it has 226 mild outliers because, since it comes from a discrete numerical variable, they are overlapping on the boxplot.

```{r}
df$log_th <- log(df$training_hours)

# Outliers
varout <- summary(df$log_th)
iqr = varout[5]-varout[2]
usout <- varout[5]+3*iqr
lsout <- varout[2]-3*iqr
umout <- varout[5]+1.5*iqr
lmout <- varout[2]-1.5*iqr

sev_out <- which((df$log_th > usout) | (df$log_th < lsout))
mild_out <- which((df$log_th > umout) | (df$log_th < lmout))

boxplot(df$log_th, horizontal = T)
abline(v=usout,col='orange', add = T)
abline(v=lsout,col='orange', add = T)
abline(v=umout,col='orange', add = T)
abline(v=lmout,col='orange', add = T)
length(sev_out) #0
length(mild_out) #17

# Distribution
hist(df$log_th, freq = F)
m = mean(df$log_th)
std = sd(df$log_th)
curve(dnorm(x,m,std),add=T, col="red")
which(df$log_th == 0)
```

We have decided to create a factor in order to check in the modelling stages whether the numerical variable or the factor is better for the model.

```{r}
df$f.training <- cut(
  df$training_hour,
  breaks = quantile(df$training_hour, probs = seq(0,1,0.2), na.rm=TRUE),
  include.lowest = TRUE
)

barplot(table(df$f.training))
summary(df$f.training)
```


## Categorical variables

### target
The variable `target` is a binary variable that defines the positive or negative outcome of the prediction.
It is clearly unbalanced, which should be taken into account in the modelling stage.

```{r}
sum(is.na(df$target))
length(unique(df$target)) #check number of unique categories -> 123
table_target <- sort(table(df$target))
head(table_target)   # cities with less frequence

df$target <- factor(df$target)
barplot(table(df$target))
```


### gender
The variable `gender` has 4508 NA and 4 unique categories: NA, Female, Male and Other.

To handle missing data, we can impute it later.

```{r}
sum(is.na(df$gender))
length(unique(df$gender)) #check number of unique categories
table_gender <- table(df$gender)
barplot(table_gender)

df$gender <- factor(df$gender)
```
### relevent_experience
The variable `relevent_experience` does not have missing data and it has 2 levels: 'Has relevant experience' and 'No relevant experience', which we rename to Yes and No.

```{r}
sum(is.na(df$relevent_experience))
length(unique(df$relevent_experience)) #check number of unique categories
table_relevent_experience<- table(df$relevent_experience)
table_relevent_experience
barplot(table_relevent_experience)

df$relevent_experience <- factor(df$relevent_experience)

df$relevent_experience <- factor(df$relevent_experience,
                                 levels = c("Has relevent experience",
                                            "No relevent experience"),
                                 labels = c("Yes", "No"))

table_relevent_experience
```
### enrolled_university
The variable `enrolled_university` has 386 NA and 4 levels: NA, 'Full time course' and 'no_enrollment' and 'Part time course'.

Like the variable `gender`, we impute it later.

```{r}
sum(is.na(df$enrolled_university))
length(unique(df$enrolled_university)) #check number of unique categories
table_enrolled_university <- table(df$enrolled_university)
table_enrolled_university
barplot(table_enrolled_university)

df$enrolled_university <- factor(df$enrolled_university)
```
### education_level
The variable `education_level` has 460 NA and 6 levels: NA, 'Graduate', 'High School','Masters','Phd' and 'Primary School'

To remove the NAs, we impute it later.



```{r}
sum(is.na(df$education_level))
length(unique(df$education_level)) #check number of unique categories
table_education_level <- table(df$education_level)
table_education_level
barplot(table_education_level)

df$education_level <- factor(df$education_level)
```
### major_discipline
The variable `major_discipline` has 2813 NA and 7 levels: NA, 'Arts', 'Business Degree','Humanities','No major','STEM' and 'Other'.

```{r}
sum(is.na(df$major_discipline))
length(unique(df$major_discipline)) #check number of unique categories
table_major_discipline <- table(df$major_discipline)
table_major_discipline
barplot(table_major_discipline)

df$major_discipline <- factor(df$major_discipline)
```
Taking into account the information in the education level variable, we saw that some categories like primary school and high school should not have a major because it is not applicable, and we decided to check whether all of the NA values are due to this reason, or if there are some other missing we should check.

After checking the missigness distribution for each education level, we confirm that all high school and primary school have NA because they're not applicable, and therefore the only actual missing values for this variable are from graduate and masters.

(Missingness NOT AT RANDOM!!!)

```{r}
table(df$education_level[is.na(df$major_discipline)])
table(df$education_level)
```


The original variable is very unbalanced, since the most typical profile is 'STEM', and the other categories are less frequent.

!!!!!!!!!! Therefore, we have decided to create a new factor with three levels: those with a STEM major, those with a non-STEM major, and those without a major.

The thing is, the category No Major is difficult to assess, given that it corresponds to graduates and masters, which should have a major, but instead of declaring their major or stating others, chose No Major.

The treatment we are going to apply for these cases is to leave them as No Major, since there is no way to tell whether they belong to STEM or not.

```{r}
# df$company_size_group <- dplyr::case_when(
#   df$company_size == "<10" ~ "Micro",
#   df$company_size %in% c("10/49", "50-99") ~ "Small",
#   df$company_size %in% c("100-500", "500-999") ~ "Medium",
#   df$company_size == "1000-4999" ~ "Large",
#   df$company_size %in% c("5000-9999", "10000") ~ "Very large"
# )
```


```{r}
# df$major_group[df$education_level %in% c("High School", "Primary School")] <- "No major"
# df$major_group[df$major_discipline == "STEM"] <- "STEM"

```


```{r}
library(dplyr)

df$major_discipline <- as.character(df$major_discipline)

df$major_discipline <- dplyr::case_when(
  df$education_level %in% c("High School", "Primary School") ~ "No major",
  df$major_discipline == "STEM" ~ "STEM",
  df$major_discipline %in% c("Other", "Arts", "Business Degree", "Humanities") ~ "Non-STEM"
)

df$major_discipline <- factor(df$major_discipline,
                                  levels = c("STEM", "Non-STEM", "No major"))

# Check
table_major_discipline <- table(df$major_discipline)
table_major_discipline
barplot(table_major_discipline)
```

### experience
The variable `experience` has 65 NA and 23 levels from <1 to >20. 

We group them in less levels.


```{r}
summary(df$experience)
sum(is.na(df$experience))
length(unique(df$experience)) #check number of unique categories
table_experience <- table(df$experience)
barplot(table_experience)
```

We make the groups by career stage, and we observe a balanced distribution and more interpretable results than with the original variable.



```{r}
exp_clean <- df$experience
exp_clean <- gsub("<1", "0", exp_clean) 
exp_clean <- gsub(">20", "21", exp_clean)

df$experience_num <- as.numeric(exp_clean)

df$experience_group <- cut(
  df$experience_num,
  breaks = c(-1, 2, 5, 10, 19, Inf),
  labels = c("Junior", "Early", "Mid", "Senior", ">20")
)

barplot(table(df$experience_group))
```


### company_size
The variable `company_size` has 5938 NAs and 9 unique categories including NA.

We can regroup this variable as well.

```{r}
sum(is.na(df$company_size))
length(unique(df$company_size)) #check number of unique categories
unique(df$company_size)
table_company_size <- table(df$company_size)
barplot(table_company_size)
```
```{r}
df$company_size_group <- dplyr::case_when(
  df$company_size == "<10" ~ "Micro",
  df$company_size %in% c("10/49", "50-99") ~ "Small",
  df$company_size %in% c("100-500", "500-999") ~ "Medium",
  df$company_size == "1000-4999" ~ "Large",
  df$company_size %in% c("5000-9999", "10000") ~ "Very large"
)

df$company_size_group <- factor(df$company_size_group, levels = c("Micro", "Small", "Medium", "Large", "Very large"))

barplot(table(df$company_size_group))
```
### company_type
The variable `company_type` has 6140 NAs and 7 unique categories.

We will impute the missing values.

We can redefine the groups as well because it's really imbalanced.
```{r}
sum(is.na(df$company_type))
length(unique(df$company_type)) #check number of unique categories
unique(df$company_type)
table_company_type
table_company_type <- table(df$company_type)
barplot(table_company_type)
```
```{r}
df$company_type_group <- dplyr::case_when(
  df$company_type == "Pvt Ltd" ~ "Private",
  df$company_type == "Public Sector" ~ "Public",
  df$company_type %in% c("Early Stage Startup", "Funded Startup") ~ "Startup",
  df$company_type %in% c("NGO", "Other") ~ "Other"
)

df$company_type_group <- factor(
  df$company_type_group,
  levels = c("Private", "Public", "Startup", "Other")
)
table(df$company_type_group)
barplot(table(df$company_type_group))
summary(df$company_type_group)
```

### last_new_job
The variable `last_new_job` has 423 NA and 7 unique categories including NA.
```{r}
sum(is.na(df$last_new_job))
length(unique(df$last_new_job)) #check number of unique categories
unique(df$last_new_job)
table_last_new_job <- table(df$last_new_job)
barplot(table_last_new_job)
```


```{r}
# Variables
check_na <- function(x) {
  na = sum(is.na(x))
  }
results <- lapply(df, check_na)
results

# Individuals
table(rowSums(is.na(df)))
```
To explore potential linear dependencies among the numerical features, we computed their correlation matrix.

```{r}
cor(df$city_development_index, df$training_hours, use = "complete.obs", method = "pearson")

```

# Missing data profiling

```{r}
# Create a variable counting the number of missing values per individual (row)
df$nb_miss <- rowSums(is.na(df))

df$nb_miss <- factor(
  cut(df$nb_miss,
      breaks = c(-1, 0, 1, 2, 3, 4 ,5, 6, 7),
      labels = c('0', '1', '2', '3', '4' ,'5', '6', '7')
  )
)

# Check distribution
table(df$nb_miss)


# Profiling 

library(FactoMineR)

dfprof <- df[,-c(7,8,13,14,15)]
names(dfprof)
res.cat <- catdes(dfprof, which(names(dfprof) == 'nb_miss'))

res.cat$quanti.var
res.cat$test.chi2
res.cat$quanti
res.cat$category


```

## Imputation
We need to impute `gender`, `enrolled_university`, `education_level`,`major_discipline`,`experience_group`, `company_size_group`,`company_type_group`,`last_new_job`.

We try to impute with imputeMCA but the results show that it imputed the NA to the higher frequency group. Because of this, we use mice to impute and the validation results show that the imputations has not changed the proportions of the different levels of the variables, especially in the variables with less NAs.

```{r}
library(missMDA)
library(FactoMineR)

cat_vars <- df[, c("gender",
                   "enrolled_university",
                   "education_level",
                   "major_discipline",
                   "experience_group",
                   "company_size_group",
                   "company_type_group",
                   "last_new_job")]

#nb_dim <- estim_ncpMCA(cat_vars)  # automatically suggests number of components
#nb_dim$ncp   # 5

#imp_mca <- imputeMCA(cat_vars, ncp = 5)
#cat_imputed <- imp_mca$completeObs

#### USING MICE?
library(mice)
res.mice <- mice(df[c(2:8,12:13,16:18)], m = 1, maxit = 5, seed = 123)
dfimp <-complete(res.mice)
summary(dfimp)

# we put imputed variables back into original df
df[, c("gender",
       "enrolled_university",
       "education_level",
       "major_discipline",
       "company_size_group",
       "company_type_group",
       "last_new_job")] <- cat_imputed

summary(df)
```

Validation:
```{r}
vars <- c("gender",
          "enrolled_university",
          "education_level",
          "major_discipline",
          "experience_group",
          "company_size_group",
          "company_type_group",
          "last_new_job")

for (v in vars) {
  cat("\n------", v, "------\n")
  
  # Proportions BEFORE (ignoring NA)
  prop_before <- prop.table(table(df[[v]], useNA = "no"))
  
  # Proportions AFTER 
  prop_after  <- prop.table(table(dfimp[[v]]))
  
  print(round(prop_before, 4))
  print(round(prop_after, 4))
}

```




# 2. Profiling and Feature Selection




# 3. Modeling using numeric variables using transformations if needed



# 4. Residual analysis: unusual and influent data filtering




# 5. Adding factor main effects to the best model containing numeric variables





# 6. Residual analysis: unusual and influent data filtering





# 7. Adding factor main effects and interactions (limit your statement to order 2) to the best model containing numeric variables




# 8. Final Residual analysis: unusual and influent data filtering. Iterative process could be needed




# 9. Goodness of fit and Model Interpretation
